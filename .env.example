# This is an example configuration file for the CodexCLI Proxy.
# To use it, copy this file to a new file named `.env` and fill in the values.

# --- REQUIRED ---

# The full URL of your target OpenAI-compatible API endpoint.
# This is the most important setting. The proxy will forward requests here.
# Example for Azure OpenAI: AZURE_ENDPOINT="https://<your-resource-name>.openai.azure.com/openai/deployments/<your-deployment-name>/chat/completions?api-version=<your-api-version>"
AZURE_ENDPOINT=""

# The API key for your target endpoint.
AZURE_API_KEY=""


# --- OPTIONAL ---

# If set, the proxy will override the model name in all incoming requests with this value.
# If left blank or commented out, the proxy will use the model specified by the client.
# Example: FORCED_MODEL="gpt-4-turbo"
FORCED_MODEL="gpt-4"

# The port on which the local proxy server will run.
# The default is 8888.
PORT=8888

# The maximum number of times the proxy will retry a failed request to the target API.
# This is useful for handling rate limits (429 errors) and transient network issues.
# The default is 10.
RETRY_ATTEMPTS=10

# The base number of seconds to wait between retries. The actual wait time is
# progressive, calculated as `attempt_number * RETRY_WAIT_SECONDS`.
# For example, with the default of 15, the first retry waits 15s, the second 30s, etc.
# The default is 15.
RETRY_WAIT_SECONDS=15
